{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e746950c-753b-4ca6-a8ff-d1529cd93e85",
   "metadata": {},
   "source": [
    "# EDA For Offensive Language Classification\n",
    "for now im trying to only deal with englishm trying to replicate the english offensive classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1836c8-ca65-4cf8-ab26-dc5c3f29ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/balamurugan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae92f4-6cdd-4041-8e82-5455cc47e905",
   "metadata": {},
   "source": [
    "### Lets import the tamil dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7930175c-ce96-4a0e-a684-23fce74c8d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Not_offensive', 'not-Tamil', 'Offensive_Targeted_Insult_Other',\n",
       "       'Offensive_Targeted_Insult_Group', 'Offensive_Untargetede',\n",
       "       'Offensive_Targeted_Insult_Individual'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/tamil_offensive_full_train.csv')\n",
    "\n",
    "df[\"Labels\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619222f8-422b-436a-9c0f-5d6a9f431dfc",
   "metadata": {},
   "source": [
    "## Lets try to preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1127fc6-525f-4138-b853-b66b62ade0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    stw = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stw])\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def preprocess(df):\n",
    "    mapClass = {'Not_offensive':0, 'not-Tamil':1,'Offensive_Targeted_Insult_Other':2,'Offensive_Targeted_Insult_Group':2, 'Offensive_Untargetede':3,'Offensive_Targeted_Insult_Individual':2}\n",
    "    df['Text'] = df['Text'].apply(clean)\n",
    "    df = df[df['Text'] != '']\n",
    "    df[\"Labels\"] = df[\"Labels\"].apply(lambda x : mapClass[x])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93c7972e-b4e6-4bc9-88ec-37efa4996d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/19b63m0164d7n0v0p6t_gr980000gn/T/ipykernel_46980/394402801.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Labels\"] = df[\"Labels\"].apply(lambda x : mapClass[x])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie vara level la erika poguthu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love ajith kumar vivegam movie inki mjy bht achi lgi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>padam nalla comedy padama irukum polaye</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>karthick subburaj anne intha padam vetri adaya unagalukku ennudaya valthukkal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ippo intha trailer ah parkuravana oru like podunga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35133</th>\n",
       "      <td>inda pathi like thala fans um thandrukanga da mooditu poo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35134</th>\n",
       "      <td>trending number idhukku nammalam karanamnu sollumbodhe oru thani gethu dhan ya kshatrian da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35135</th>\n",
       "      <td>movie script super athuvum hip hop tamizha music vera</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35136</th>\n",
       "      <td>k likes k likes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35137</th>\n",
       "      <td>aaloo le lo kanda le lo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Text  \\\n",
       "0                                                                movie vara level la erika poguthu   \n",
       "1                                             love ajith kumar vivegam movie inki mjy bht achi lgi   \n",
       "2                                                          padam nalla comedy padama irukum polaye   \n",
       "3                    karthick subburaj anne intha padam vetri adaya unagalukku ennudaya valthukkal   \n",
       "5                                               ippo intha trailer ah parkuravana oru like podunga   \n",
       "...                                                                                            ...   \n",
       "35133                                    inda pathi like thala fans um thandrukanga da mooditu poo   \n",
       "35134  trending number idhukku nammalam karanamnu sollumbodhe oru thani gethu dhan ya kshatrian da   \n",
       "35135                                        movie script super athuvum hip hop tamizha music vera   \n",
       "35136                                                                              k likes k likes   \n",
       "35137                                                                      aaloo le lo kanda le lo   \n",
       "\n",
       "       Labels  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "5           0  \n",
       "...       ...  \n",
       "35133       2  \n",
       "35134       0  \n",
       "35135       0  \n",
       "35136       0  \n",
       "35137       1  \n",
       "\n",
       "[29896 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc9c4be4-26c7-4f12-862f-f1aef56e7f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "344853ef-d66a-4af7-8274-3a2924acf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f486d3c2-bf83-4892-ba57-70263868053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81b04219-e938-48cd-a4b3-845f5ba2e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Text\"]\n",
    "y = df[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb99372b-ac8a-4c0b-9b93-8ef18927948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    6, 2340,  411],\n",
       "       [   0,    0,    0, ..., 6212, 1802, 8321],\n",
       "       [   0,    0,    0, ...,  279,   90, 1093],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  942,   55,    5],\n",
       "       [   0,    0,    0, ...,   28,   20,   28],\n",
       "       [   0,    0,    0, ..., 3047,  435,  743]], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 20000  \n",
    "maxlen = 200  \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "text = X.tolist()  \n",
    "def tokenizer(text)\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequence = tokenizer.texts_to_sequences(text)\n",
    "    X = pad_sequences(sequence, maxlen=maxlen)\n",
    "    return X\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d645e0f-5a19-45d7-816d-4b1219bf68b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23916 Training sequences\n",
      "5980 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_pad, x_val_pad, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(x_train_pad), \"Training sequences\")\n",
    "print(len(x_val_pad), \"Validation sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fc3e106-0943-4f03-a6e6-ea370552e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train_pad, maxlen=maxlen)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val_pad, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5b5e0aa-4047-444f-bcae-b3fad1963ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64   \n",
    "num_heads = 4    \n",
    "ff_dim = 128     \n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d146ea9-aea3-4ff0-a4d4-20f47cdc1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 383ms/step - accuracy: 0.6916 - loss: 0.9685 - val_accuracy: 0.7048 - val_loss: 0.9023\n",
      "Epoch 2/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 413ms/step - accuracy: 0.7093 - loss: 0.9190 - val_accuracy: 0.7048 - val_loss: 0.8604\n",
      "Epoch 3/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 462ms/step - accuracy: 0.7115 - loss: 0.7592 - val_accuracy: 0.7423 - val_loss: 0.6734\n",
      "Epoch 4/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 400ms/step - accuracy: 0.7841 - loss: 0.5567 - val_accuracy: 0.7433 - val_loss: 0.6564\n",
      "Epoch 5/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 344ms/step - accuracy: 0.8246 - loss: 0.4672 - val_accuracy: 0.7497 - val_loss: 0.6941\n",
      "Epoch 6/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 347ms/step - accuracy: 0.8501 - loss: 0.3884 - val_accuracy: 0.7212 - val_loss: 0.7453\n",
      "Epoch 7/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 367ms/step - accuracy: 0.8737 - loss: 0.3343 - val_accuracy: 0.7410 - val_loss: 0.8858\n",
      "Epoch 8/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 415ms/step - accuracy: 0.9051 - loss: 0.2677 - val_accuracy: 0.7231 - val_loss: 1.0207\n",
      "Epoch 9/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 456ms/step - accuracy: 0.9333 - loss: 0.1980 - val_accuracy: 0.7201 - val_loss: 1.2146\n",
      "Epoch 10/10\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 422ms/step - accuracy: 0.9437 - loss: 0.1638 - val_accuracy: 0.7331 - val_loss: 1.3109\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b393a-f3a7-4c85-82ca-0deaccb4a381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tenenv)",
   "language": "python",
   "name": "jupyter-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
